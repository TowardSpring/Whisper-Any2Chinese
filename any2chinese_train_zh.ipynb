{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune the whisper to realize the any2chinese task"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whisper模型是OpenAI推出的语音识别模型，该模型支持多种语言的语音识别，将其转录为对应语言文本。同时，该模型还有翻译功能，能将多种语言的语音转录为英文文本。在Whisper模型诞生之后，很多开源项目对其进行了微调或改进，使得模型能够在小语种上语音识别能力得到了进一步的增强，并且在C语言编译、PEFT等技术的加持下，Whisper模型现在可以在小设备上加速运行。\n",
    "\n",
    "但是，当前，还鲜有看到有项目进一步的开发Whisper对新语种语音转录功能，或者进一步微调模型使其支持将各种语言的语音直接转录为中文文本。显然，这将进一步的发掘模型的潜力。\n",
    "\n",
    "本教程借助transformers包收录的Whisper模型，在其基础上对该模型的tokenizer进行了修改。这样使得Whisper模型具备转录新语种以及直接转录为中文文本的能力。本教程将该方法命名为Whisper-Any2Chinese模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-26 14:51:15.860174: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login, login\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from pprint import pprint\n",
    "from transformers import WhisperFeatureExtractor\n",
    "import os\n",
    "from tokenization_whisper import WhisperTokenizer\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "from datasets import Audio\n",
    "import torch\n",
    "from config.whisper_config import MODEL_CONFIG_FILE_ROOT_PATH, DATASET_NAME_OR_PATH, LANGUAGE, LANGUAGE_ABBR, MODEL_NAME_OR_PATH, TASK, NEW_TOKENS, OUTPUT_DIR\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "import evaluate\n",
    "from transformers import WhisperForConditionalGeneration,Seq2SeqTrainingArguments\n",
    "from peft import prepare_model_for_int8_training,LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model,PeftModel, PeftConfig\n",
    "from transformers import Seq2SeqTrainer, TrainerCallback, TrainingArguments, TrainerState, TrainerControl,WhisperForConditionalGeneration, Seq2SeqTrainer\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在微调模型之前，你需要准备好训练数据。训练数据的格式以及数据的元信息文件如本教程的“./data/*”中的两个数据集所示。其中“en_mini”表示英文数据集（包含对应的转录中文），“ug_mini”表示维语数据集（包含对应的转录中文）。你可以根据自己的需求，准备好自己的数据集。本教程的数据来自于commonvoice数据集，你可以在[这里](https://commonvoice.mozilla.org/)下载到该数据集。（只做demo，因此本教程的数据集只包含了很少的数据）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51be8b656524edca7e15e4dc159c59c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset audiofolder/ug_mini to /home/towardspring/.cache/huggingface/datasets/audiofolder/ug_mini-3bff67ba257ec961/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd6b9d1a758403b9543d225dca53337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9809f9164a4f18a00c08ebcec7a28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4999d53af2a8422aa9dfbd9c858c0e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99ad54eefc14255b83abd5c07e40d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d739641a23444ff8abbac458abfb9145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2980945f5a44d98f951cea4ce9b392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e877ea24f4d949469fd38d7d9fcd0e67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9b6dcb1cbb4204b040b1760aca784a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f64bb4dfef48f0a9ad6e119c37c8c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0499bcb3072642b888d14c72f4c1fe5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf925832f804ed781d3a2374cf44537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c05b255d10546cb91a2cacd1c01d1b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset audiofolder downloaded and prepared to /home/towardspring/.cache/huggingface/datasets/audiofolder/ug_mini-3bff67ba257ec961/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77cb29278f72491399219c401280da76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset audiofolder (/home/towardspring/.cache/huggingface/datasets/audiofolder/ug_mini-3bff67ba257ec961/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c07f8c257e4f99a65a29ca5c86200a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset audiofolder (/home/towardspring/.cache/huggingface/datasets/audiofolder/ug_mini-3bff67ba257ec961/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'sentence', 'chinese'],\n",
      "        num_rows: 55\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'sentence', 'chinese'],\n",
      "        num_rows: 12\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['audio', 'sentence', 'chinese'],\n",
      "        num_rows: 15\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(DATASET_NAME_OR_PATH, split=\"train\")\n",
    "common_voice[\"test\"] = load_dataset(DATASET_NAME_OR_PATH, split=\"test\")\n",
    "common_voice[\"validation\"] = load_dataset(DATASET_NAME_OR_PATH, split=\"validation\")\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOEDL_NAME_OR_PATH:/home/towardspring/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0\n",
      "Model has been loaded.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "print(f'MOEDL_NAME_OR_PATH:{MODEL_NAME_OR_PATH}')\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(MODEL_NAME_OR_PATH, language=LANGUAGE_ABBR, task=TASK)\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME_OR_PATH, language=LANGUAGE_ABBR, task=TASK)\n",
    "print('Model has been loaded.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whisper本身是不支持添加新语种或者多语言中文转录功能。因此，我们需要在微调之前，添加新的语种标签以及中文转录翻译提示标签。例如，若想实现对维吾尔语的转录，我们需要向tokenizer模块添加<|ug|>标签；若想实现对多语种的中文转录翻译功能，需要向tokenizer模块添加<|translate-chinese|>标签。本教材只对英语语音转录翻译中文文本做出示范，您可以根据您的需要添加任意语音或翻译标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_special_tokens': ['<|endoftext|>', '<|startoftranscript|>', '<|en|>', '<|zh|>', '<|de|>', '<|es|>', '<|ru|>', '<|ko|>', '<|fr|>', '<|ja|>', '<|pt|>', '<|tr|>', '<|pl|>', '<|ca|>', '<|nl|>', '<|ar|>', '<|sv|>', '<|it|>', '<|id|>', '<|hi|>', '<|fi|>', '<|vi|>', '<|he|>', '<|uk|>', '<|el|>', '<|ms|>', '<|cs|>', '<|ro|>', '<|da|>', '<|hu|>', '<|ta|>', '<|no|>', '<|th|>', '<|ur|>', '<|hr|>', '<|bg|>', '<|lt|>', '<|la|>', '<|mi|>', '<|ml|>', '<|cy|>', '<|sk|>', '<|te|>', '<|fa|>', '<|lv|>', '<|bn|>', '<|sr|>', '<|az|>', '<|sl|>', '<|kn|>', '<|et|>', '<|mk|>', '<|br|>', '<|eu|>', '<|is|>', '<|hy|>', '<|ne|>', '<|mn|>', '<|bs|>', '<|kk|>', '<|sq|>', '<|sw|>', '<|gl|>', '<|mr|>', '<|pa|>', '<|si|>', '<|km|>', '<|sn|>', '<|yo|>', '<|so|>', '<|af|>', '<|oc|>', '<|ka|>', '<|be|>', '<|tg|>', '<|sd|>', '<|gu|>', '<|am|>', '<|yi|>', '<|lo|>', '<|uz|>', '<|fo|>', '<|ht|>', '<|ps|>', '<|tk|>', '<|nn|>', '<|mt|>', '<|sa|>', '<|lb|>', '<|my|>', '<|bo|>', '<|tl|>', '<|mg|>', '<|as|>', '<|tt|>', '<|haw|>', '<|ln|>', '<|ha|>', '<|ba|>', '<|jw|>', '<|su|>', '<|translate|>', '<|transcribe|>', '<|startoflm|>', '<|startofprev|>', '<|nocaptions|>', '<|notimestamps|>', '<|translate-chinese|>'], 'bos_token': {'content': '<|endoftext|>', 'lstrip': False, 'normalized': True, 'rstrip': False, 'single_word': False}, 'eos_token': {'content': '<|endoftext|>', 'lstrip': False, 'normalized': True, 'rstrip': False, 'single_word': False}, 'pad_token': '<|endoftext|>', 'unk_token': {'content': '<|endoftext|>', 'lstrip': False, 'normalized': True, 'rstrip': False, 'single_word': False}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "tokenizer.add_tokens(NEW_TOKENS)\n",
    "tokenizer.save_pretrained(MODEL_CONFIG_FILE_ROOT_PATH)   # Note: you need to change the path (config.whisper_config.py) to your own path\n",
    "\n",
    "special_tokens_file = os.path.join(MODEL_CONFIG_FILE_ROOT_PATH, 'special_tokens_map.json')\n",
    "with open(special_tokens_file, 'r') as f:\n",
    "    special_tokens_map = json.load(f)\n",
    "print(special_tokens_map)\n",
    "additional_special_tokens = special_tokens_map['additional_special_tokens']\n",
    "# print(len(additional_special_tokens))\n",
    "for new_item in NEW_TOKENS:\n",
    "    if new_item not in additional_special_tokens:\n",
    "        additional_special_tokens.append(new_item)\n",
    "\n",
    "# save to special_tokens_map.json\n",
    "special_tokens_map['additional_special_tokens'] = additional_special_tokens\n",
    "json_str = json.dumps(special_tokens_map,ensure_ascii=False)\n",
    "\n",
    "with open(special_tokens_file, 'w') as f:\n",
    "    f.write(json_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在开始训练之前，我们需要对tokenizer进行修改，使其支持新加入的提示标签。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在完成必须的修改之后，我们需要利用载入数据并对tokenizer进行测试，检查语言编码与任务编码是否正确。若获得如样例所示的样本，则证明tokenizer修改正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids = [50258, 51866, 51865, 50363, 913, 250, 25395, 8225, 15868, 98, 9254, 15368, 171, 120, 234, 913, 251, 17015, 36269, 7093, 1543, 50257]\n",
      "ids = [50258, 51866, 51865, 50363, 913, 250, 25395, 8225, 15868, 98, 9254, 15368, 171, 120, 234, 913, 251, 17015, 36269, 7093, 1543, 50257]\n",
      "Input: “我不能滥用这个，”女孩想。\n",
      "Decoded with special: <|startoftranscript|><|ug|><|translate-chinese|><|notimestamps|>“我不能滥用这个，”女孩想。<|endoftext|>\n",
      "Decoded without special: <|ug|>“我不能滥用这个，”女孩想。\n",
      "Are equal: False\n"
     ]
    }
   ],
   "source": [
    "input_str = common_voice[\"train\"][0][\"chinese\"]\n",
    "labels = tokenizer(input_str).input_ids\n",
    "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(f\"Input: {input_str}\")\n",
    "print(f\"Decoded with special: {decoded_with_special}\")\n",
    "print(f\"Decoded without special: {decoded_str}\")\n",
    "print(f\"Are equal: {input_str == decoded_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据处理\n",
    "pprint(common_voice[\"train\"][1])\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "pprint(common_voice[\"train\"][1])\n",
    "\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"chinese\"]).input_ids   # Note: you need to change the language to your own language\n",
    "    return batch\n",
    "\n",
    "\n",
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=21)\n",
    "\n",
    "common_voice[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training parameters\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        print(f'input_features:{input_features}, size of input_features[0]:{len(input_features[0])}')\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "# metric = evaluate.load(\"wer\")\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME_OR_PATH, load_in_8bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluate\n",
    "# metric = evaluate.load(\"wer\")\n",
    "\n",
    "\n",
    "# def compute_metrics(pred):\n",
    "#     pred_ids = pred.predictions\n",
    "#     label_ids = pred.label_ids\n",
    "\n",
    "#     # replace -100 with the pad_token_id\n",
    "#     label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "#     # we do not want to group tokens when computing the metrics\n",
    "#     pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "#     label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "#     wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "#     return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = prepare_model_for_int8_training(model, output_embedding_layer_name=\"proj_out\")\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "def make_inputs_require_grad(module, input, output):\n",
    "    output.requires_grad_(True)\n",
    "\n",
    "model.model.encoder.conv1.register_forward_hook(make_inputs_require_grad)\n",
    "\n",
    "config = LoraConfig(r=32, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-3,\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=500,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit = 6,\n",
    "    fp16=True,\n",
    "    per_device_eval_batch_size=8,\n",
    "    generation_max_length=128,\n",
    "    logging_steps=100,\n",
    "    # max_steps=6000, # only for testing purposes, remove this from your final run :)s\n",
    "    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n",
    "    label_names=[\"labels\"],  # same reason as above\n",
    ")\n",
    "\n",
    "# This callback helps to save only the adapter weights and remove the base model weights.\n",
    "class SavePeftModelCallback(TrainerCallback):\n",
    "    def on_save(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
    "\n",
    "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
    "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
    "\n",
    "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
    "        if os.path.exists(pytorch_model_path):\n",
    "            os.remove(pytorch_model_path)\n",
    "        return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    callbacks=[SavePeftModelCallback],\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.device_count())\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"  # disable logging to Weights & Biases for this run\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"trained_model/whisper-small-en2chinese\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
